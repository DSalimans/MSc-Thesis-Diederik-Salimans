{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperbolic & hierarchical image classification"
      ],
      "metadata": {
        "id": "LFJxhR1HkbLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "import sys\n",
        "import re\n",
        "import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "from torchvision.io import ImageReadMode, read_image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "\n",
        "# installing geoopt\n",
        "!pip install -q git+https://github.com/geoopt/geoopt.git\n",
        "! [ ! -f mobius_linear_example.py ] && wget -q https://raw.githubusercontent.com/geoopt/geoopt/master/examples/mobius_linear_example.py\n",
        "import geoopt\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "import math\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from   torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve8sTdh3lT9U",
        "outputId": "a45235f4-574f-4399-cfee-825dac42a07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for geoopt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_mapping(dataset):\n",
        "  if dataset == 'cifar100':\n",
        "    class_mapping = {\n",
        "    0: \"apple\",\n",
        "    1: \"aquarium_fish\",\n",
        "    2: \"baby\",\n",
        "    3: \"bear\",\n",
        "    4: \"beaver\",\n",
        "    5: \"bed\",\n",
        "    6: \"bee\",\n",
        "    7: \"beetle\",\n",
        "    8: \"bicycle\",\n",
        "    9: \"bottle\",\n",
        "    10: \"bowl\",\n",
        "    11: \"boy\",\n",
        "    12: \"bridge\",\n",
        "    13: \"bus\",\n",
        "    14: \"butterfly\",\n",
        "    15: \"camel\",\n",
        "    16: \"can\",\n",
        "    17: \"castle\",\n",
        "    18: \"caterpillar\",\n",
        "    19: \"cattle\",\n",
        "    20: \"chair\",\n",
        "    21: \"chimpanzee\",\n",
        "    22: \"clock\",\n",
        "    23: \"cloud\",\n",
        "    24: \"cockroach\",\n",
        "    25: \"couch\",\n",
        "    26: \"crab\",\n",
        "    27: \"crocodile\",\n",
        "    28: \"cup\",\n",
        "    29: \"dinosaur\",\n",
        "    30: \"dolphin\",\n",
        "    31: \"elephant\",\n",
        "    32: \"flatfish\",\n",
        "    33: \"forest\",\n",
        "    34: \"fox\",\n",
        "    35: \"girl\",\n",
        "    36: \"hamster\",\n",
        "    37: \"house\",\n",
        "    38: \"kangaroo\",\n",
        "    39: \"keyboard\",\n",
        "    40: \"lamp\",\n",
        "    41: \"lawn_mower\",\n",
        "    42: \"leopard\",\n",
        "    43: \"lion\",\n",
        "    44: \"lizard\",\n",
        "    45: \"lobster\",\n",
        "    46: \"man\",\n",
        "    47: \"maple\",\n",
        "    48: \"motorcycle\",\n",
        "    49: \"mountain\",\n",
        "    50: \"mouse\",\n",
        "    51: \"mushroom\",\n",
        "    52: \"oak\",\n",
        "    53: \"orange\",\n",
        "    54: \"orchid\",\n",
        "    55: \"otter\",\n",
        "    56: \"palm\",\n",
        "    57: \"pear\",\n",
        "    58: \"pickup_truck\",\n",
        "    59: \"pine\",\n",
        "    60: \"plain\",\n",
        "    61: \"plate\",\n",
        "    62: \"poppy\",\n",
        "    63: \"porcupine\",\n",
        "    64: \"possum\",\n",
        "    65: \"rabbit\",\n",
        "    66: \"raccoon\",\n",
        "    67: \"ray\",\n",
        "    68: \"road\",\n",
        "    69: \"rocket\",\n",
        "    70: \"rose\",\n",
        "    71: \"sea\",\n",
        "    72: \"seal\",\n",
        "    73: \"shark\",\n",
        "    74: \"shrew\",\n",
        "    75: \"skunk\",\n",
        "    76: \"skyscraper\",\n",
        "    77: \"snail\",\n",
        "    78: \"snake\",\n",
        "    79: \"spider\",\n",
        "    80: \"squirrel\",\n",
        "    81: \"streetcar\",\n",
        "    82: \"sunflower\",\n",
        "    83: \"sweet_pepper\",\n",
        "    84: \"table\",\n",
        "    85: \"tank\",\n",
        "    86: \"telephone\",\n",
        "    87: \"television\",\n",
        "    88: \"tiger\",\n",
        "    89: \"tractor\",\n",
        "    90: \"train\",\n",
        "    91: \"trout\",\n",
        "    92: \"tulip\",\n",
        "    93: \"turtle\",\n",
        "    94: \"wardrobe\",\n",
        "    95: \"whale\",\n",
        "    96: \"willow\",\n",
        "    97: \"wolf\",\n",
        "    98: \"woman\",\n",
        "    99: \"worm\"}\n",
        "  return class_mapping\n",
        "\n",
        "def get_embedding(dataset, hierarch):\n",
        "    if dataset == 'cifar100':\n",
        "      if hierarch == 'balanced':\n",
        "        embedding = torch.load('/content/balanced.pth.best')\n",
        "      if hierarch == 'base':\n",
        "        embedding = torch.load('/content/base.pth.best')\n",
        "      if hierarch == 'expert':\n",
        "        embedding = torch.load('/content/expert.pth.best')\n",
        "      if hierarch == 'random':\n",
        "        embedding = torch.load('/content/random.pth.best')\n",
        "      if hierarch == 'root':\n",
        "        embedding = torch.load('/content/root.pth.best')\n",
        "      if hierarch == 'size':\n",
        "        embedding = torch.load('/content/size.pth.best')\n",
        "    return embedding\n",
        "\n",
        "def get_translator(mapping, embedding):\n",
        "  embedding_list = embedding['objects']\n",
        "  tensors = {}\n",
        "  for i in range(len(mapping)):\n",
        "    map = mapping[i]\n",
        "    index = [j for j, x in enumerate(embedding_list) if x == map+'.n01']\n",
        "    tensors[i] = embedding['embeddings'][index]\n",
        "  return tensors\n",
        "\n",
        "def translating(inputs, translator):\n",
        "  tensors = []\n",
        "  for item in inputs:\n",
        "    tensors.append(translator[item.tolist()])\n",
        "\n",
        "  concatenated_tensor = torch.cat(tensors, dim=0)\n",
        "  reshaped_tensors = concatenated_tensor.reshape(len(inputs), 64)\n",
        "  return reshaped_tensors\n",
        "\n",
        "def get_leaf_nodes_tensors(inputs, translator):\n",
        "  tensors = []\n",
        "  for item in inputs:\n",
        "    tensors.append(translator[item])\n",
        "  return tensors\n",
        "\n",
        "DATASET = \"cifar100\"\n",
        "HIERARCH = \"balanced\"\n",
        "embedding = get_embedding(DATASET, HIERARCH)\n",
        "class_mapping = get_class_mapping(DATASET)\n",
        "translator = get_translator(class_mapping, embedding)\n",
        "\n",
        "leaf_nodes_tensors = get_leaf_nodes_tensors(range(0,100,1), translator)\n",
        "leaf_nodes_tensors = torch.cat(leaf_nodes_tensors, dim=0)\n",
        "\n",
        "np.save(\"prototypes_\" + HIERARCH + '_64dim', leaf_nodes_tensors, allow_pickle=True, fix_imports=True)"
      ],
      "metadata": {
        "id": "2sHw9ncNBsm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HIERARCHY CLASS\n",
        "def add_relations_from_csv(csv_file, hierarchy):\n",
        "    with open(csv_file, 'r') as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        next(csv_reader)  # Skip the header row if present\n",
        "\n",
        "        for row in csv_reader:\n",
        "            if len(row) >= 2:\n",
        "                child, parent = row[:2]  # Extract the first two columns\n",
        "                hierarchy.add_relation(child, parent)\n",
        "\n",
        "class Hierarchy:\n",
        "  def __init__(self):\n",
        "      self.hierarchy = {}\n",
        "\n",
        "  def add_relation(self, child, parent):\n",
        "      if child not in self.hierarchy:\n",
        "          self.hierarchy[child] = []\n",
        "      self.hierarchy[child].append(parent)\n",
        "\n",
        "  def get_parents(self, child):\n",
        "      if child in self.hierarchy:\n",
        "          return self.hierarchy[child]\n",
        "      return []\n",
        "\n",
        "  def get_siblings(self, child):\n",
        "      parents = self.get_parents(child)\n",
        "      siblings = []\n",
        "      for parent in parents:\n",
        "          siblings.extend(self.get_children(parent))\n",
        "      if child in siblings:\n",
        "          siblings.remove(child)\n",
        "      return siblings\n",
        "\n",
        "  def get_children(self, parent):\n",
        "      children = []\n",
        "      for child, parents in self.hierarchy.items():\n",
        "          if parent in parents:\n",
        "              children.append(child)\n",
        "      return children\n",
        "\n",
        "  def get_cousins(self, child):\n",
        "      parents = self.get_parents(child)\n",
        "      cousins = []\n",
        "      for parent in parents:\n",
        "          siblings = self.get_siblings(parent)\n",
        "          cousins.extend(siblings)\n",
        "      if child in cousins:\n",
        "          cousins.remove(child)\n",
        "      return cousins\n",
        "\n",
        "  def get_grandparents(self, child):\n",
        "      parents = self.get_parents(child)\n",
        "      grandparents = []\n",
        "      for parent in parents:\n",
        "          grandparents.extend(self.get_parents(parent))\n",
        "      grandparents = list(set(grandparents))  # Remove duplicates\n",
        "      return grandparents\n",
        "\n",
        "  def find_lowest_common_ancestor(self, class1, class2):\n",
        "      if class1 not in self.hierarchy or class2 not in self.hierarchy:\n",
        "          return None\n",
        "\n",
        "      path1 = self.get_path_to_root(class1)\n",
        "      path2 = self.get_path_to_root(class2)\n",
        "\n",
        "      lowest_common_ancestor = None\n",
        "      for i in range(min(len(path1), len(path2))):\n",
        "          if path1[i] == path2[i]:\n",
        "              lowest_common_ancestor = path1[i]\n",
        "          else:\n",
        "              break\n",
        "\n",
        "      return lowest_common_ancestor\n",
        "\n",
        "  def get_path_to_root(self, class_name):\n",
        "      path = []\n",
        "      current = class_name\n",
        "\n",
        "      while current != \"root.n01\":\n",
        "          parents = self.get_parents(current)\n",
        "          if not parents:\n",
        "              break\n",
        "          current = parents[0]\n",
        "          path.append(current)\n",
        "\n",
        "      return path[::-1]  # Reverse the path\n",
        "\n",
        "  def find_step_distance(self, class1, class2):\n",
        "      if class1 == class2:\n",
        "          return 0\n",
        "      if class1 == \"root.n01\":\n",
        "          return len(self.get_path_to_root(class2))\n",
        "      if class2 == \"root.n01\":\n",
        "          return len(self.get_path_to_root(class1))\n",
        "\n",
        "      path1 = self.get_path_to_root(class1)\n",
        "      path2 = self.get_path_to_root(class2)\n",
        "\n",
        "      # Check if class1 is an ancestor of class2\n",
        "      if class1 in path2:\n",
        "          return len(path2) - path2.index(class1)\n",
        "\n",
        "      # Check if class2 is an ancestor of class1\n",
        "      if class2 in path1:\n",
        "          return len(path1) - path1.index(class2)\n",
        "\n",
        "      lowest_common_ancestor = self.find_lowest_common_ancestor(class1, class2)\n",
        "      if lowest_common_ancestor:\n",
        "          distance1 = len(path1) - path1.index(lowest_common_ancestor)\n",
        "          distance2 = len(path2) - path2.index(lowest_common_ancestor)\n",
        "          return distance1+distance2\n",
        "      return None\n",
        "\n",
        "# HIERARCHY FUNCS\n",
        "hierarchy = Hierarchy()\n",
        "csv_file = DATASET + '_' + HIERARCH + '.csv'\n",
        "add_relations_from_csv(csv_file, hierarchy)\n",
        "\n",
        "# Check if two classes are siblings\n",
        "def is_sibling(class1, class2):\n",
        "    siblings = hierarchy.get_siblings(class1)\n",
        "    return class2 in siblings\n",
        "\n",
        "# Check if two classes are cousins\n",
        "def is_cousin(class1, class2):\n",
        "    uncles = hierarchy.get_cousins(class1)\n",
        "    for item in uncles:\n",
        "      if is_parent(item, class2):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Check if one class is a parent of another classb\n",
        "def is_parent(class1, class2):\n",
        "    children = hierarchy.get_children(class1)\n",
        "    return class2 in children\n",
        "\n",
        "# Check if one class is a grandparent of another class\n",
        "def is_grandparent(class1, class2):\n",
        "    grandparents = hierarchy.get_grandparents(class2)\n",
        "    return class1 in grandparents\n",
        "\n",
        "# find lowest common ancestor\n",
        "def lca(class1, class2):\n",
        "    lca = hierarchy.find_lowest_common_ancestor(class1, class2)\n",
        "    return lca\n",
        "\n",
        "# calculate step distances between classes\n",
        "def step_distance(class1, class2):\n",
        "  distance = hierarchy.find_step_distance(class1, class2)\n",
        "  return distance\n",
        "\n",
        "def step_distance_from_lca(class1, class2):\n",
        "  lowest_ancestor = lca(class1, class2)\n",
        "  distance = step_distance(class1, lowest_ancestor)\n",
        "  return distance"
      ],
      "metadata": {
        "id": "WSkzBSLsB3QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def get_model(network, dims, prototypes=None, tau=1.0, curvature=1.00):\n",
        "    if network == \"resnet34\":\n",
        "        model = resnet34(dims, prototypes, tau, curvature)\n",
        "    elif network == \"resnet50\":\n",
        "        model = resnet50(dims, prototypes)\n",
        "    return model.cuda()\n",
        "\n",
        "def get_cifar100(batch_size, ex_class=-1):\n",
        "    cmean = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "    cstd  = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "\n",
        "    # Transforms.\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cmean, cstd)\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cmean, cstd)\n",
        "    ])\n",
        "\n",
        "    # Get train loader.\n",
        "    train_data = torchvision.datasets.CIFAR100(root=\"data/\", train=True, transform=transform_train,download=True)\n",
        "    train_loader = DataLoader(train_data, shuffle=True, num_workers=1, batch_size=batch_size)\n",
        "\n",
        "    # Get test loader.\n",
        "    test_data = torchvision.datasets.CIFAR100(root=\"data/\", train=False, transform=transform_test)\n",
        "    test_loader = DataLoader(test_data, shuffle=False, num_workers=1, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "M_e8gt9plSdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        #residual function\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BasicBlock.expansion))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BottleNeck.expansion))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_block, num_classes=64, prototypes=None, tau=1.0, curvature=1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True))\n",
        "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
        "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
        "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
        "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, 64)\n",
        "\n",
        "        # Fixed final layer.\n",
        "        self.prototypes = prototypes\n",
        "        self.ball = geoopt.PoincareBallExact(c=curvature)\n",
        "        self.tau = tau\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, y=None, l=None):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2_x(output)\n",
        "        output = self.conv3_x(output)\n",
        "        output = self.conv4_x(output)\n",
        "        output = self.conv5_x(output)\n",
        "        output = self.avg_pool(output)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output = self.fc(output)\n",
        "        output = self.ball.expmap0(output)\n",
        "\n",
        "        if self.prototypes is not None:\n",
        "            output = self.ball.expmap0(output)\n",
        "            output = -self.ball.dist(output[:,None,:], self.prototypes) * self.tau # output[:,None,:]\n",
        "        return output\n",
        "\n",
        "def resnet34(dims, prototypes, tau, curvature):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=dims, prototypes=prototypes, tau=tau, curvature=curvature)\n",
        "\n",
        "def resnet50(dims, prototypes):\n",
        "    return ResNet(BottleNeck, [3, 4, 6, 3], num_classes=dims, prototypes=prototypes)"
      ],
      "metadata": {
        "id": "D2mynjNCky6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"Set seed\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "def train(model, train_loader, loss_function, epoch, prototypes=None):\n",
        "    model.train()\n",
        "    total = 0\n",
        "    avgloss = 0.\n",
        "    itercount = 0.\n",
        "    n_steps_per_epoch = len(train_loader.dataset) / train_loader.batch_size\n",
        "    # Iterate over all samples.\n",
        "    for batch_index, (images, labels) in enumerate(train_loader):\n",
        "        total += len(images)\n",
        "\n",
        "        # Images and labels to GPU.\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation.\n",
        "        outputs = model(images)\n",
        "        # tensors = translating(labels, translator).cuda()\n",
        "        # loss = loss_function(outputs, tensors)\n",
        "        loss = loss_function(outputs, labels)\n",
        "\n",
        "        # Backward propagation.\n",
        "        avgloss += loss\n",
        "        itercount += 1\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return avgloss / itercount\n",
        "\n",
        "def test(model, test_loader, loss_function):\n",
        "    model.eval()\n",
        "\n",
        "    step_distances = []\n",
        "    step_distances_from_lca = []\n",
        "    correct = 0.0\n",
        "    sibling = 0.0\n",
        "    cousin = 0.0\n",
        "    total_test_loss = 0.0\n",
        "    itercount = 0.0\n",
        "    for (images, labels) in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # Forward propagation.\n",
        "        outputs = model(images)\n",
        "\n",
        "        # tensors = translating(labels, translator).cuda()\n",
        "        # test_loss = loss_function(outputs, tensors)\n",
        "        test_loss = loss_function(outputs, labels)\n",
        "\n",
        "        _, predictions = outputs.max(1)\n",
        "\n",
        "        # Evaluation\n",
        "        for pred, label in zip(predictions, labels):\n",
        "          pred, label = pred.item(), label.item()\n",
        "\n",
        "          if pred == label: #accuracy\n",
        "            correct += 1\n",
        "          if is_sibling(class_mapping[pred] + '.n01', class_mapping[label] + '.n01'): #siblings\n",
        "            sibling += 1\n",
        "          if is_cousin(class_mapping[pred] + '.n01', class_mapping[label] + '.n01'): #cousins\n",
        "            cousin += 1\n",
        "          step_distances_from_lca.append(step_distance_from_lca(class_mapping[pred]+'.n01', class_mapping[label]+'.n01'))\n",
        "          step_distances.append(step_distance(class_mapping[pred]+'.n01', class_mapping[label]+'.n01'))\n",
        "\n",
        "        # loss logging\n",
        "        itercount += 1\n",
        "        total_test_loss += test_loss.item()\n",
        "\n",
        "    sibling_acc = sibling / len(test_loader.dataset)\n",
        "    cousin_acc = cousin / len(test_loader.dataset)\n",
        "    acc = correct / len(test_loader.dataset)\n",
        "    loss = total_test_loss / itercount\n",
        "    return acc, loss, sibling_acc, cousin_acc, step_distances_from_lca, step_distances\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Parse user arguments.\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"-n\", dest=\"network\", default=\"resnet34\", type=str)\n",
        "    parser.add_argument(\"-d\", dest=\"dataset\", default=\"cifar100\", type=str)\n",
        "    parser.add_argument(\"-b\", dest=\"batch_size\", default=512, type=int)\n",
        "    parser.add_argument(\"-l\", dest=\"learning_rate\", default=0.1, type=float)\n",
        "    parser.add_argument(\"-u\", dest=\"use_scheduler\", default=1, type=int)\n",
        "    parser.add_argument(\"-s\", dest=\"scale_factor\", default=0.95, type=float)\n",
        "    parser.add_argument(\"-t\", dest=\"tau\", default=10, type=float)\n",
        "    parser.add_argument(\"-e\", dest=\"epochs\", default=200, type=int)\n",
        "    parser.add_argument(\"-f\", dest=\"resfile\", default=\"\", type=str)\n",
        "    parser.add_argument(\"--prot\", dest=\"prototype_file\", default=\"prototypes_\"+ HIERARCH +\"_64dim.npy\", type=str)\n",
        "    parser.add_argument(\"--c\", dest=\"curvature\", default=1, type=float)\n",
        "    parser.add_argument(\"--seed\", dest=\"seed\", default=42, type=int)\n",
        "    args = parser.parse_args()\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Get data.\n",
        "    if args.dataset == \"cifar100\":\n",
        "        train_loader, test_loader = get_cifar100(args.batch_size, 0)\n",
        "        nr_classes = 100\n",
        "\n",
        "    # prototypes = None\n",
        "    prototypes = (torch.from_numpy(np.load(args.prototype_file)).float()).cuda()\n",
        "    # prototypes = leaf_nodes_tensors.cuda()\n",
        "    prototypes = F.normalize(prototypes, p=2, dim=1)\n",
        "    prototypes = prototypes * args.scale_factor / math.sqrt(args.curvature)\n",
        "\n",
        "    # Get network, loss function, and optimizer.\n",
        "    model = get_model(args.network, nr_classes, prototypes, args.tau, args.curvature).cuda()\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    # loss_function = hyperbolic_loss\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,100,150], gamma=0.2)\n",
        "\n",
        "    print(\"------------------------------------------------\")\n",
        "    # Perform training and periodic testing.\n",
        "    for epoch in range(args.epochs):\n",
        "\n",
        "        # Train for one epoch.\n",
        "        train_loss = train(model, train_loader, loss_function, epoch, prototypes)\n",
        "\n",
        "        # Test.\n",
        "        if epoch % 10 == 0 or epoch == args.epochs -1:\n",
        "            acc, test_loss, sibling_acc, cousin_acc, step_distances_from_lca, step_distances = test(model, test_loader, loss_function)\n",
        "\n",
        "            print('[', epoch, ']')\n",
        "            print('Train Loss:  ', round(train_loss.item(), 4))\n",
        "            print('Test Loss:   ', round(test_loss, 4))\n",
        "            print('Accuracy:    ', round(acc * 100, 2), '%')\n",
        "            print('Sibling acc: ', round(sibling_acc * 100, 2), '%')\n",
        "            print('Cousin acc:  ', round(cousin_acc * 100, 2), '%')\n",
        "            print('Steps:       ', round(np.mean(step_distances), 4))\n",
        "            print('LCA steps:   ', round(np.mean(step_distances_from_lca), 4))\n",
        "            print(\"------------------------------------------------\")\n",
        "\n",
        "        # Learning rate scheduler update.\n",
        "        if args.use_scheduler == 1:\n",
        "            train_scheduler.step()\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5e1RDwdkZ_0",
        "outputId": "e683cb3b-6c87-41ab-e256-98e7c4e24c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "------------------------------------------------\n",
            "[ 0 ]\n",
            "Train Loss:   5.2442\n",
            "Test Loss:    4.8027\n",
            "Accuracy:     1.54 %\n",
            "Sibling acc:  3.71 %\n",
            "Cousin acc:   4.49 %\n",
            "Steps:        9.5604\n",
            "LCA steps:    4.7956\n",
            "------------------------------------------------\n",
            "[ 10 ]\n",
            "Train Loss:   4.0849\n",
            "Test Loss:    4.1118\n",
            "Accuracy:     4.92 %\n",
            "Sibling acc:  13.81 %\n",
            "Cousin acc:   4.03 %\n",
            "Steps:        8.6216\n",
            "LCA steps:    4.36\n",
            "------------------------------------------------\n",
            "[ 20 ]\n",
            "Train Loss:   4.5925\n",
            "Test Loss:    4.6326\n",
            "Accuracy:     1.03 %\n",
            "Sibling acc:  2.14 %\n",
            "Cousin acc:   2.7 %\n",
            "Steps:        10.098\n",
            "LCA steps:    5.0593\n",
            "------------------------------------------------\n",
            "[ 30 ]\n",
            "Train Loss:   3.4397\n",
            "Test Loss:    3.5603\n",
            "Accuracy:     11.19 %\n",
            "Sibling acc:  24.5 %\n",
            "Cousin acc:   2.62 %\n",
            "Steps:        7.1598\n",
            "LCA steps:    3.6918\n",
            "------------------------------------------------\n",
            "[ 40 ]\n",
            "Train Loss:   2.9765\n",
            "Test Loss:    3.419\n",
            "Accuracy:     14.0 %\n",
            "Sibling acc:  28.97 %\n",
            "Cousin acc:   2.31 %\n",
            "Steps:        6.5244\n",
            "LCA steps:    3.4022\n",
            "------------------------------------------------\n",
            "[ 50 ]\n",
            "Train Loss:   2.0756\n",
            "Test Loss:    2.3306\n",
            "Accuracy:     29.82 %\n",
            "Sibling acc:  39.19 %\n",
            "Cousin acc:   1.17 %\n",
            "Steps:        4.0478\n",
            "LCA steps:    2.3221\n",
            "------------------------------------------------\n",
            "[ 60 ]\n",
            "Train Loss:   1.8169\n",
            "Test Loss:    2.494\n",
            "Accuracy:     35.93 %\n",
            "Sibling acc:  31.28 %\n",
            "Cousin acc:   1.17 %\n",
            "Steps:        4.062\n",
            "LCA steps:    2.3903\n",
            "------------------------------------------------\n",
            "[ 70 ]\n",
            "Train Loss:   2.0378\n",
            "Test Loss:    2.8041\n",
            "Accuracy:     31.12 %\n",
            "Sibling acc:  30.07 %\n",
            "Cousin acc:   1.73 %\n",
            "Steps:        4.6158\n",
            "LCA steps:    2.6191\n",
            "------------------------------------------------\n",
            "[ 80 ]\n",
            "Train Loss:   1.8113\n",
            "Test Loss:    2.4818\n",
            "Accuracy:     37.71 %\n",
            "Sibling acc:  31.35 %\n",
            "Cousin acc:   1.26 %\n",
            "Steps:        3.8714\n",
            "LCA steps:    2.3128\n",
            "------------------------------------------------\n",
            "[ 90 ]\n",
            "Train Loss:   1.5485\n",
            "Test Loss:    2.632\n",
            "Accuracy:     41.65 %\n",
            "Sibling acc:  27.3 %\n",
            "Cousin acc:   1.11 %\n",
            "Steps:        3.816\n",
            "LCA steps:    2.3245\n",
            "------------------------------------------------\n",
            "[ 100 ]\n",
            "Train Loss:   1.2006\n",
            "Test Loss:    2.425\n",
            "Accuracy:     47.81 %\n",
            "Sibling acc:  25.88 %\n",
            "Cousin acc:   0.99 %\n",
            "Steps:        3.2978\n",
            "LCA steps:    2.127\n",
            "------------------------------------------------\n",
            "[ 110 ]\n",
            "Train Loss:   0.9441\n",
            "Test Loss:    2.9734\n",
            "Accuracy:     53.17 %\n",
            "Sibling acc:  20.45 %\n",
            "Cousin acc:   1.24 %\n",
            "Steps:        3.1896\n",
            "LCA steps:    2.1265\n",
            "------------------------------------------------\n",
            "[ 120 ]\n",
            "Train Loss:   0.9034\n",
            "Test Loss:    3.2965\n",
            "Accuracy:     55.29 %\n",
            "Sibling acc:  17.58 %\n",
            "Cousin acc:   1.29 %\n",
            "Steps:        3.2014\n",
            "LCA steps:    2.1536\n",
            "------------------------------------------------\n",
            "[ 130 ]\n",
            "Train Loss:   0.8912\n",
            "Test Loss:    3.6233\n",
            "Accuracy:     55.83 %\n",
            "Sibling acc:  15.92 %\n",
            "Cousin acc:   1.17 %\n",
            "Steps:        3.2962\n",
            "LCA steps:    2.2064\n",
            "------------------------------------------------\n",
            "[ 140 ]\n",
            "Train Loss:   0.8883\n",
            "Test Loss:    3.6537\n",
            "Accuracy:     56.7 %\n",
            "Sibling acc:  15.82 %\n",
            "Cousin acc:   1.2 %\n",
            "Steps:        3.1942\n",
            "LCA steps:    2.1641\n",
            "------------------------------------------------\n",
            "[ 150 ]\n",
            "Train Loss:   0.8069\n",
            "Test Loss:    3.5054\n",
            "Accuracy:     59.15 %\n",
            "Sibling acc:  15.01 %\n",
            "Cousin acc:   1.15 %\n",
            "Steps:        3.023\n",
            "LCA steps:    2.103\n",
            "------------------------------------------------\n",
            "[ 160 ]\n",
            "Train Loss:   0.7205\n",
            "Test Loss:    3.8367\n",
            "Accuracy:     60.98 %\n",
            "Sibling acc:  13.63 %\n",
            "Cousin acc:   1.16 %\n",
            "Steps:        2.9426\n",
            "LCA steps:    2.0811\n",
            "------------------------------------------------\n",
            "[ 170 ]\n",
            "Train Loss:   0.7049\n",
            "Test Loss:    4.0505\n",
            "Accuracy:     61.76 %\n",
            "Sibling acc:  12.48 %\n",
            "Cousin acc:   1.2 %\n",
            "Steps:        2.9646\n",
            "LCA steps:    2.0999\n",
            "------------------------------------------------\n",
            "[ 180 ]\n",
            "Train Loss:   0.6862\n",
            "Test Loss:    4.207\n",
            "Accuracy:     62.16 %\n",
            "Sibling acc:  11.8 %\n",
            "Cousin acc:   1.37 %\n",
            "Steps:        2.9632\n",
            "LCA steps:    2.1032\n",
            "------------------------------------------------\n",
            "[ 190 ]\n",
            "Train Loss:   0.684\n",
            "Test Loss:    4.2818\n",
            "Accuracy:     62.46 %\n",
            "Sibling acc:  11.82 %\n",
            "Cousin acc:   1.18 %\n",
            "Steps:        2.9452\n",
            "LCA steps:    2.0972\n",
            "------------------------------------------------\n",
            "[ 199 ]\n",
            "Train Loss:   0.6763\n",
            "Test Loss:    4.3938\n",
            "Accuracy:     62.7 %\n",
            "Sibling acc:  11.4 %\n",
            "Cousin acc:   1.29 %\n",
            "Steps:        2.9502\n",
            "LCA steps:    2.1021\n",
            "------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "MzyFTFHNSH4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a method to create a random hierarchichal structure. A csv is then manually created using the result\n",
        "def generate_random_integer_list(sum_value, min_length=2, min_value=2, max_overall_value=5):\n",
        "    random_list = []\n",
        "    while sum_value > 0:\n",
        "        max_value = min(sum_value, min_value + (sum_value - min_length + 1))\n",
        "        if max_value < min_value:\n",
        "            break\n",
        "        rand_int = random.randint(min_value, min(max_value, max_overall_value))\n",
        "        random_list.append(rand_int)\n",
        "        sum_value -= rand_int\n",
        "\n",
        "    while len(random_list) < min_length:\n",
        "        rand_int = random.randint(min_value, sum_value)\n",
        "        random_list.append(rand_int)\n",
        "        sum_value -= rand_int\n",
        "\n",
        "    return random_list\n",
        "\n",
        "depth = 3\n",
        "leaves_vals = []\n",
        "total = 0\n",
        "results_list = []\n",
        "\n",
        "leaves = generate_random_integer_list(100, max_overall_value=6)\n",
        "for entry in leaves:\n",
        "    curr = total\n",
        "    leaves_vals.append([i for i in range(curr, curr+entry)])\n",
        "    total += entry\n",
        "\n",
        "while results_list == []:\n",
        "    try:\n",
        "        results_list.append(leaves_vals)\n",
        "        for i in range(depth):\n",
        "            results_list.insert(0, generate_random_integer_list(len(results_list[0]), max_overall_value=5))\n",
        "    except ValueError:\n",
        "        results_list = []\n",
        "        pass\n",
        "\n",
        "print(results_list)\n"
      ],
      "metadata": {
        "id": "4dk9dEKQQ29u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5p-Rl09kC55p",
        "5FUfUIVIzWYU",
        "IhjyFmcIrjxQ",
        "nclsiioheQfD",
        "LuxtGTdggwxr",
        "yk6wuy7xz7Ev",
        "Tju6cvpsfCsV",
        "4LmQMSGWpb6e",
        "BXoDoZ7l3tg4",
        "GXwtVaAA3v5I",
        "pAAhZBWa5Bbz",
        "BYOyUXoR9qGq",
        "lURYb3Q0BP1V",
        "vUUJHhvQBV1I",
        "2lKWoXpy-fXs",
        "HXkx2RqrfU6U",
        "GTSzyw363F6f",
        "L-hHzPpv3Jl6",
        "rS74AatA281A",
        "vTKHVj5m5P3O",
        "gNxs_oG747IN",
        "bqVeGAxZ4vpx",
        "YoYZ1l0k07RF",
        "28-kCfK60-fA",
        "7jXkCjc61ByU",
        "3QPfRLdkkNx0"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}